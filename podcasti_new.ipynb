{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import vertexai\n",
    "import os\n",
    "# Path to your service account key file\n",
    "key_path = r\"stoked-forest-447811-u4-ecf33505a9e7.json\"\n",
    "\n",
    "# Set the e\n",
    "# nvironment variable\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
    "LOCATION=\"us-central1\"\n",
    "vertexai.init(location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"playground\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from google.cloud import texttospeech\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "@tool\n",
    "def text_to_speech(text: str, voice: str, output_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts text to speech using Google Cloud's Text-to-Speech API, saving the audio file locally with a dynamic filename.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to convert to speech.\n",
    "        voice (str): The voice to use for speech synthesis. This should be in the format:\n",
    "                     - \"en-US-Wavenet-D\" (for US English, WaveNet voice)\n",
    "                     - \"en-GB-Standard-A\" (for UK English, Standard voice)\n",
    "                     - See https://cloud.google.com/text-to-speech/docs/voices for more options.\n",
    "        output_filename (str): The name for the output audio file, including file extension (e.g., 'output.mp3').\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the saved audio file.\n",
    "    \"\"\"\n",
    "    # Set up Google Cloud credentials (ensure environment variable is set)\n",
    "    \n",
    "\n",
    "    # Initialize the client\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    # Set the text input\n",
    "    synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "    # Build the voice request\n",
    "    voice_config = texttospeech.VoiceSelectionParams(\n",
    "        language_code=voice[:5],  # Extract language code (e.g., \"en-US\" from \"en-US-Wavenet-D\")\n",
    "        name=voice,  # Use the full voice name\n",
    "    )\n",
    "\n",
    "    # Set the audio configuration\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    # Perform the text-to-speech request\n",
    "    response = client.synthesize_speech(\n",
    "        input=synthesis_input, voice=voice_config, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    # Save the audio to a file\n",
    "    speech_file_path = Path(output_filename)\n",
    "    with open(speech_file_path, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "\n",
    "    return str(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from pydub import AudioSegment\n",
    "from typing import List\n",
    "\n",
    "@tool\n",
    "def edit_podcast_audio(segments: List[str], pauses_between_segments: int = 1000, output_filename: str = \"final_podcast_episode.mp3\") -> str:\n",
    "    \"\"\"\n",
    "    Edits a podcast episode by combining audio segments with specified pauses between them, ensuring consistent volume.\n",
    "\n",
    "    Args:\n",
    "        segments (List[str]): List of paths to audio segment files.\n",
    "        pauses_between_segments (int): Duration of pause between segments in milliseconds. Default is 1000.\n",
    "        output_filename (str): The name for the output podcast file, including file extension (e.g., 'episode.mp3').\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the saved podcast episode.\n",
    "    \"\"\"\n",
    "    podcast_episode = AudioSegment.silent(duration=0)  # Initialize an empty audio segment\n",
    "\n",
    "    for segment_path in segments:\n",
    "        segment = AudioSegment.from_file(segment_path)  # Load the segment\n",
    "        podcast_episode += AudioSegment.silent(duration=pauses_between_segments) + segment  # Append with pause\n",
    "\n",
    "    podcast_episode = podcast_episode.normalize()  # Normalize volume\n",
    "    podcast_episode.export(output_filename, format='mp3')  # Export the edited podcast\n",
    "\n",
    "    return output_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools we want to use\n",
    "tools = [\n",
    "    tavily_tool,  # Built-in search tool via Tavily\n",
    "    text_to_speech,  # Our custom text to speech tool\n",
    "    edit_podcast_audio # Audio Mix Tool\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_24252\\1653953709.py:2: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  tool_executor = ToolExecutor(tools)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model = \"gemini-2.0-flash-exp\", temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from google.generativeai import GenerativeModel  # Import Gemini's GenerativeModel\n",
    "\n",
    "def create_agent(gemini_model, tools, system_message: str):\n",
    "    \"\"\"\n",
    "    Create an agent using Google's Gemini model.\n",
    "\n",
    "    Args:\n",
    "        gemini_model: An instance of Google's GenerativeModel (Gemini).\n",
    "        tools: A list of tools the agent can use.\n",
    "        system_message (str): The system message to guide the agent's behavior.\n",
    "\n",
    "    Returns:\n",
    "        A configured agent that can interact with Gemini and use tools.\n",
    "    \"\"\"\n",
    "    # Convert tools to a format Gemini can understand (e.g., descriptions)\n",
    "    tool_descriptions = \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools])\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools:\\n{tool_descriptions}\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_descriptions=tool_descriptions)\n",
    "\n",
    "    # Bind the prompt to the Gemini model\n",
    "    agent = prompt | gemini_model\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import operator\n",
    "from typing import Annotated, List, Sequence, Tuple, TypedDict, Union\n",
    "\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# This defines the object that is passed between each node\n",
    "# in the graph. We will create different nodes for each agent and tool\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "  result = agent.invoke(state)\n",
    "  if isinstance(result, FunctionMessage):\n",
    "    pass\n",
    "  else:\n",
    "    result = HumanMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "  return {\n",
    "      \"messages\": [result],\n",
    "      \"sender\": name\n",
    "  }\n",
    "\n",
    "#Podcast Planner Agent and Node\n",
    "podcast_planner_agent = create_agent(\n",
    "    model,\n",
    "    [tavily_tool, text_to_speech],\n",
    "    system_message=\"\"\"\n",
    "You are tasked with creating a structured script for a podcast episode. The script should consist of a series of interactions between the host and the guest based on the provided topic and information from the research.\n",
    "\n",
    "For each part of the dialogue, clearly specify whether it's the host speaking or the guest. Also, assign a suitable voice model for text-to-speech conversion for each segment. Use the following voice models based on the character:\n",
    "\n",
    "- Host segments: Use the 'alloy' voice model.\n",
    "- Guest segments: Use the 'fable' voice model.\n",
    "\n",
    "The output should be a list where each item is a dictionary with keys 'speaker', 'text', and 'voice', indicating the speaker (host or guest), their line of dialogue, and the voice model to use.\n",
    "\n",
    "Example output format:\n",
    "[\n",
    "    {\"speaker\": \"host\", \"text\": \"Welcome to our podcast, where we explore the latest in technology.\", \"voice\": \"alloy\"},\n",
    "    {\"speaker\": \"guest\", \"text\": \"Thank you for having me, it's great to be here to share my experiences.\", \"voice\": \"fable\"},\n",
    "    {\"speaker\": \"host\", \"text\": \"Can you tell us about your current project?\", \"voice\": \"alloy\"},\n",
    "    {\"speaker\": \"guest\", \"text\": \"Certainly! I've been working on a new AI platform that...\", \"voice\": \"fable\"},\n",
    "    ...\n",
    "]\n",
    "\n",
    "Your task is to generate a similar structured script, ensuring each dialogue segment between the host and guest is well-defined and allocates the appropriate voice model for the text-to-speech conversion process.\n",
    "\"\"\"\n",
    "\n",
    ")\n",
    "\n",
    "podcast_planner_node = functools.partial(agent_node, agent=podcast_planner_agent, name=\"Podcast Plannner\")\n",
    "\n",
    "#Research Agent\n",
    "\n",
    "research_agent = create_agent(\n",
    "    model,\n",
    "    [tavily_tool],\n",
    "    system_message=\"You should provide accurate data for both the Podcast Planner to use\"\n",
    "\n",
    ")\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "#Editor Agent\n",
    "editor_agent = create_agent(\n",
    "    model,\n",
    "    [tavily_tool],\n",
    "    system_message=\"\"\"\n",
    "You are the Editor, tasked with a critical review of the podcast script before it goes to audio production. Your review must focus on three key areas:\n",
    "\n",
    "1. Flow and Clarity: Ensure that the dialogue between the host and guest flows naturally and is clear for listeners. The script must be optimized for text-to-speech conversion, paying close attention to pronunciation, pacing, and expression.\n",
    "\n",
    "2. File System Uniqueness: Verify that the filenames suggested for each audio segment are unique and follow a logical naming convention. This is crucial to avoid overwriting files and to ensure seamless integration in the final podcast episode.\n",
    "\n",
    "3. Content Quality and Rewrites: Assess the content for its informational value, engagement, and suitability for the podcast's audience. You have the authority to rewrite parts of the dialogue to enhance clarity, engagement, or factual accuracy. Your goal is to refine the script to a point where it translates effectively into an engaging audio experience.\n",
    "\n",
    "After your review, you may either approve the script for audio production or make necessary adjustments. If adjustments are made, clearly indicate the changes and provide updated filenames if necessary. Your input will directly influence the quality of the final podcast episode.\n",
    "\"\"\"\n",
    ")\n",
    "editor_node = functools.partial(agent_node, agent=editor_agent, name=\"Editor\")\n",
    "\n",
    "\n",
    "#Audio Mixer Agent\n",
    "audio_agent = create_agent(\n",
    "    model,\n",
    "    [text_to_speech, edit_podcast_audio],\n",
    "    system_message=\"\"\"\n",
    "You are responsible for producing the final audio for the podcast episode. Take the structured script provided by the Podcast Planner, which contains segments marked with 'speaker' (either 'host' or 'guest'), the 'text' for each segment, and the 'voice' model to use.\n",
    "\n",
    "For each segment, use the 'text_to_speech' tool to generate audio, specifying the 'text' and 'voice' as provided. Ensure each segment is saved as a separate audio file.\n",
    "\n",
    "After generating all segments, use the 'edit_podcast_audio' tool to combine these audio files into one seamless podcast episode. The audio files should be combined in the order they are provided in the script, with appropriate pauses between segments to simulate a natural conversation flow.\n",
    "\n",
    "Your output should be the path to the final combined podcast episode audio file.\n",
    "\"\"\"\n",
    "\n",
    ")\n",
    "audio_node = functools.partial(agent_node, agent=audio_agent, name=\"Audio\")\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_node(state):\n",
    "    \"\"\"This runs tools in the graph\n",
    "\n",
    "    It takes in an agent action and calls that tool and returns the result.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    tool_input = json.loads(\n",
    "        last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "    )\n",
    "    # We can pass single-arg inputs by value\n",
    "    if len(tool_input) == 1 and \"__arg1\" in tool_input:\n",
    "        tool_input = next(iter(tool_input.values()))\n",
    "    tool_name = last_message.additional_kwargs[\"function_call\"][\"name\"]\n",
    "    action = ToolInvocation(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_input,\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(\n",
    "        content=f\"{tool_name} response: {str(response)}\", name=action.tool\n",
    "    )\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state):\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"function_call\" in last_message.additional_kwargs:\n",
    "        # The previus agent is invoking a tool\n",
    "        return \"call_tool\"\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return \"end\"\n",
    "    return \"continue\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Assuming AgentState, tool_node, and all other nodes are defined elsewhere in your code.\n",
    "\n",
    "# Initialize the graph with the state type\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes for each part of the workflow\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"Podcast Planner\", podcast_planner_node)\n",
    "workflow.add_node(\"Editor\", editor_node)  # Editor node added\n",
    "workflow.add_node(\"Audio\", audio_node)\n",
    "\n",
    "# Node to handle tool invocations\n",
    "workflow.add_node(\"call_tool\", tool_node)\n",
    "\n",
    "# Define the flow from Researcher to Podcast Planner\n",
    "workflow.add_conditional_edges(\n",
    "    \"Researcher\",\n",
    "    router,\n",
    "    {\"continue\": \"Podcast Planner\", \"call_tool\": \"call_tool\", \"end\": END},\n",
    ")\n",
    "\n",
    "# Define the flow from Podcast Planner to Editor\n",
    "workflow.add_conditional_edges(\n",
    "    \"Podcast Planner\",\n",
    "    router,\n",
    "    {\"continue\": \"Editor\", \"call_tool\": \"call_tool\", \"end\": END},\n",
    ")\n",
    "\n",
    "# Define the flow from Editor to Audio\n",
    "workflow.add_conditional_edges(\n",
    "    \"Editor\",\n",
    "    router,\n",
    "    {\"continue\": \"Audio\", \"call_tool\": \"call_tool\", \"end\": END},\n",
    ")\n",
    "\n",
    "# Define the flow for the Audio node\n",
    "workflow.add_conditional_edges(\n",
    "    \"Audio\",\n",
    "    router,\n",
    "    {\"continue\": END, \"call_tool\": \"call_tool\", \"end\": END},  # After Audio, the process ends or calls a tool\n",
    ")\n",
    "\n",
    "# Define how the graph transitions back from calling a tool\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_tool\",\n",
    "    lambda state: state[\"sender\"],  # Routes back to the original agent who invoked the tool\n",
    "    {\n",
    "        \"Researcher\": \"Researcher\",\n",
    "        \"Podcast Planner\": \"Podcast Planner\",\n",
    "        \"Editor\": \"Editor\",\n",
    "        \"Audio\": \"Audio\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Set the entry point for the graph to \"Researcher\"\n",
    "workflow.set_entry_point(\"Researcher\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Researcher': {'messages': [HumanMessage(content='```tool_code\\n{\"tavily_search_results_json\": {\"query\": \"2024 Super Bowl, Patrick Mahomes, Travis Kelce, Taylor Swift, halftime show\"}}\\n```', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, name='Researcher', id='run-5419c168-bfb4-49cd-9b29-84af3c3f2004-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 182, 'output_tokens': 42, 'total_tokens': 224, 'input_token_details': {'cache_read': 0}})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Start the graph with an initial message that represents the podcast topic or question\n",
    "initial_message = HumanMessage(\n",
    "    content=\"Research the upcoming 2024 superbowl, some sub plots about Pat Mahomes, Kelce and Taylor swift as well as well as the halftime performance\"\n",
    ")\n",
    "\n",
    "# Stream through the graph, processing each step according to the defined workflow\n",
    "for state in graph.stream(\n",
    "    {\"messages\": [initial_message], \"sender\": \"User\"},  # Initial state with the message from the user\n",
    "    {\"recursion_limit\": 150}  # Set a limit to prevent infinite loops\n",
    "):\n",
    "    print(state)  # Print out the state at each step to observe the progress\n",
    "    print(\"----\")\n",
    "\n",
    "# This loop will go through the research, planning, audio production, and potentially tool invocation steps,\n",
    "# depending on how your nodes and router logic are set up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
