{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijGzTHJJUCPY"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9003470a8d3b"
      },
      "source": [
        "# Imagen 3 Image Editing\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/imagen3_editing.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fvision%2Fgetting-started%2Fimagen3_editing.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/vision/getting-started/imagen3_editing.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/imagen3_editing.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/imagen3_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/imagen3_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/imagen3_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/imagen3_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/imagen3_editing.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1KDmM_PBAXz"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Katie Nguyen](https://github.com/katiemn) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkHPv2myT2cx"
      },
      "source": [
        "## Overview\n",
        "\n",
        "### Imagen 3\n",
        "\n",
        "Imagen 3 on Vertex AI brings Google's state of the art generative AI capabilities to application developers. Imagen 3 is Google's highest quality text-to-image model to date. It's capable of creating images with astonishing detail. Thus, developers have more control when building next-generation AI products that transform their imagination into high quality visual assets. Learn more about [Imagen on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrkcqHrrwMAo"
      },
      "source": [
        "In this tutorial, you will learn how to use the Vertex AI SDK for Python to interact with Imagen 3 and modify existing images with mask-based editing in the following modes:\n",
        "\n",
        "- Inpainting\n",
        "- Product background editing\n",
        "- Outpainting\n",
        "- Controlled customization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r11Gu7qNgx1p"
      },
      "source": [
        "## Get started\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell to authenticate your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "\n",
        "import vertexai\n",
        "import os\n",
        "# Path to your service account key file\n",
        "key_path = r\"stoked-forest-447811-u4-ecf33505a9e7.json\"\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
        "LOCATION=\"asia-east1\"\n",
        "vertexai.init( location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua6PDqB1iBSb"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yTiDo0lRh6sc"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.vision_models import Image,ImageGenerationModel,ReferenceImage,ControlReferenceImage,RawReferenceImage,MaskReferenceImage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr2Y3lFwKW1M"
      },
      "source": [
        "### Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r_38e5rRKB6s"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "from PIL import Image as PIL_Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Gets the image bytes from a PIL Image object.\n",
        "def get_bytes_from_pil(image: PIL_Image) -> bytes:\n",
        "    byte_io_png = io.BytesIO()\n",
        "    image.save(byte_io_png, \"PNG\")\n",
        "    return byte_io_png.getvalue()\n",
        "\n",
        "\n",
        "# Pads an image for outpainting.\n",
        "def pad_to_target_size(\n",
        "    source_image,\n",
        "    target_size=(1536, 1536),\n",
        "    mode=\"RGB\",\n",
        "    vertical_offset_ratio=0,\n",
        "    horizontal_offset_ratio=0,\n",
        "    fill_val=255,\n",
        "):\n",
        "    orig_image_size_w, orig_image_size_h = source_image.size\n",
        "    target_size_w, target_size_h = target_size\n",
        "\n",
        "    insert_pt_x = (target_size_w - orig_image_size_w) // 2 + int(\n",
        "        horizontal_offset_ratio * target_size_w\n",
        "    )\n",
        "    insert_pt_y = (target_size_h - orig_image_size_h) // 2 + int(\n",
        "        vertical_offset_ratio * target_size_h\n",
        "    )\n",
        "    insert_pt_x = min(insert_pt_x, target_size_w - orig_image_size_w)\n",
        "    insert_pt_y = min(insert_pt_y, target_size_h - orig_image_size_h)\n",
        "\n",
        "    if mode == \"RGB\":\n",
        "        source_image_padded = PIL_Image.new(\n",
        "            mode, target_size, color=(fill_val, fill_val, fill_val)\n",
        "        )\n",
        "    elif mode == \"L\":\n",
        "        source_image_padded = PIL_Image.new(mode, target_size, color=(fill_val))\n",
        "    else:\n",
        "        raise ValueError(\"source image mode must be RGB or L.\")\n",
        "\n",
        "    source_image_padded.paste(source_image, (insert_pt_x, insert_pt_y))\n",
        "    return source_image_padded\n",
        "\n",
        "\n",
        "# Pads and resizes image and mask to the same target size.\n",
        "def pad_image_and_mask(\n",
        "    image_vertex: Image,\n",
        "    mask_vertex: Image,\n",
        "    target_size,\n",
        "    vertical_offset_ratio,\n",
        "    horizontal_offset_ratio,\n",
        "):\n",
        "    image_vertex.thumbnail(target_size)\n",
        "    mask_vertex.thumbnail(target_size)\n",
        "\n",
        "    image_vertex = pad_to_target_size(\n",
        "        image_vertex,\n",
        "        target_size=target_size,\n",
        "        mode=\"RGB\",\n",
        "        vertical_offset_ratio=vertical_offset_ratio,\n",
        "        horizontal_offset_ratio=horizontal_offset_ratio,\n",
        "        fill_val=0,\n",
        "    )\n",
        "    mask_vertex = pad_to_target_size(\n",
        "        mask_vertex,\n",
        "        target_size=target_size,\n",
        "        mode=\"L\",\n",
        "        vertical_offset_ratio=vertical_offset_ratio,\n",
        "        horizontal_offset_ratio=horizontal_offset_ratio,\n",
        "        fill_val=255,\n",
        "    )\n",
        "    return image_vertex, mask_vertex\n",
        "\n",
        "\n",
        "def display_images(original_image, modified_image) -> None:\n",
        "    fig, axis = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    axis[0].imshow(original_image._pil_image)\n",
        "    axis[0].set_title(\"Original Image\")\n",
        "    axis[1].imshow(modified_image._pil_image)\n",
        "    axis[1].set_title(\"Edited Image\")\n",
        "    for ax in axis:\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLmwIj2RD0Fx"
      },
      "source": [
        "### Load the image models\n",
        "\n",
        "Imagen 3 Generation: `imagen-3.0-generate-001`\n",
        "\n",
        "Imagen 3 Editing: `imagen-3.0-capability-001`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F-gd2ypQhh7K"
      },
      "outputs": [],
      "source": [
        "generation_model = ImageGenerationModel.from_pretrained(\"imagen-3.0-generate-001\")\n",
        "\n",
        "edit_model = ImageGenerationModel.from_pretrained(\"imagen-3.0-capability-001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f64d92aef6cb"
      },
      "source": [
        "### Inpainting insert \n",
        "\n",
        "In these examples you will specify a targeted area to apply edits to. In the case of inpainting insert, you'll use a mask area to add image content to an existing image. Start by generating an image using Imagen 3. Then create two ```ReferenceImage``` objects, one for your reference image and one for your mask. For the ```MaskReferenceImage``` set ```image=None```, this will allow for automatic mask detection based on the specified ```mask_mode```.\n",
        "\n",
        "When generating images you can also set the `safety_filter_level` and `person_generation` parameters accordingly:\n",
        "* `person_generation`: Allow (All ages), Allow (Adults only), Don't allow\n",
        "* `safety_filter_level`: Block most, Block some, Block few"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wwZBW0UW-PiW"
      },
      "outputs": [
        {
          "ename": "NotFound",
          "evalue": "404 POST https://asia-east1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/asia-east1/publishers/google/models/imagen-3.0-capability-001@default:predict?%24alt=json%3Benum-encoding%3Dint: Image editing failed with the following error: imagen-3.0-capability-001 is unavailable.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m raw_ref_image \u001b[38;5;241m=\u001b[39m RawReferenceImage(image\u001b[38;5;241m=\u001b[39mgenerated_image[\u001b[38;5;241m0\u001b[39m], reference_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m mask_ref_image \u001b[38;5;241m=\u001b[39m MaskReferenceImage(\n\u001b[0;32m     15\u001b[0m     reference_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeground\u001b[39m\u001b[38;5;124m\"\u001b[39m, dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m edited_image \u001b[38;5;241m=\u001b[39m \u001b[43medit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medit_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43medit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minpainting-insert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mraw_ref_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ref_image\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumber_of_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_filter_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblock_some\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperson_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow_adult\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m display_images(generated_image[\u001b[38;5;241m0\u001b[39m], edited_image[\u001b[38;5;241m0\u001b[39m])\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\vertexai\\vision_models\\_vision_models.py:1217\u001b[0m, in \u001b[0;36mImageGenerationModel.edit_image\u001b[1;34m(self, prompt, base_image, mask, reference_images, negative_prompt, number_of_images, guidance_scale, edit_mode, mask_mode, segmentation_classes, mask_dilation, product_position, output_mime_type, compression_quality, language, seed, output_gcs_uri, safety_filter_level, person_generation)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21medit_image\u001b[39m(\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageGenerationResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Edits an existing image based on text prompt.\u001b[39;00m\n\u001b[0;32m   1149\u001b[0m \n\u001b[0;32m   1150\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;124;03m        An `ImageGenerationResponse` object.\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumber_of_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43medit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medit_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msegmentation_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegmentation_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_dilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproduct_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproduct_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_mime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_mime_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression_quality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_quality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_gcs_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gcs_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_watermark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Not supported for editing yet\u001b[39;49;00m\n\u001b[0;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_filter_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_filter_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperson_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperson_generation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\vertexai\\vision_models\\_vision_models.py:1022\u001b[0m, in \u001b[0;36mImageGenerationModel._generate_images\u001b[1;34m(self, prompt, negative_prompt, number_of_images, width, height, aspect_ratio, guidance_scale, seed, base_image, mask, reference_images, edit_mode, mask_mode, segmentation_classes, mask_dilation, product_position, output_mime_type, compression_quality, language, output_gcs_uri, add_watermark, safety_filter_level, person_generation)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_options:\n\u001b[0;32m   1020\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output_options\n\u001b[1;32m-> 1022\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1027\u001b[0m generated_images: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneratedImage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(response\u001b[38;5;241m.\u001b[39mpredictions):\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\google\\cloud\\aiplatform\\models.py:2426\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[1;34m(self, instances, parameters, timeout, use_raw_predict, use_dedicated_endpoint)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[0;32m   2418\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   2419\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2422\u001b[0m         model_version_id\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelVersionId\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   2423\u001b[0m     )\n\u001b[0;32m   2425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2426\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2428\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[0;32m   2433\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json_format\u001b[38;5;241m.\u001b[39mMessageToDict(prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata)\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\google\\cloud\\aiplatform_v1\\services\\prediction_service\\client.py:914\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[1;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\google\\cloud\\aiplatform_v1\\services\\prediction_service\\transports\\rest.py:1460\u001b[0m, in \u001b[0;36mPredictionServiceRestTransport._Predict.__call__\u001b[1;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;66;03m# In case of error, raise the appropriate core_exceptions.GoogleAPICallError exception\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;66;03m# subclass.\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m-> 1460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core_exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;66;03m# Return the response\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m resp \u001b[38;5;241m=\u001b[39m prediction_service\u001b[38;5;241m.\u001b[39mPredictResponse()\n",
            "\u001b[1;31mNotFound\u001b[0m: 404 POST https://asia-east1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/asia-east1/publishers/google/models/imagen-3.0-capability-001@default:predict?%24alt=json%3Benum-encoding%3Dint: Image editing failed with the following error: imagen-3.0-capability-001 is unavailable."
          ]
        }
      ],
      "source": [
        "image_prompt = \"\"\"\n",
        "a small wooden bowl with grapes and apples on a marble kitchen counter, light brown cabinets blurred in the background\n",
        "\"\"\"\n",
        "generated_image = generation_model.generate_images(\n",
        "    prompt=image_prompt,\n",
        "    number_of_images=1,\n",
        "    aspect_ratio=\"1:1\",\n",
        "    safety_filter_level=\"block_some\",\n",
        "    person_generation=\"dont_allow\",\n",
        ")\n",
        "\n",
        "edit_prompt = \"a small white ceramic bowl with lemons and limes\"\n",
        "raw_ref_image = RawReferenceImage(image=generated_image[0], reference_id=0)\n",
        "mask_ref_image = MaskReferenceImage(\n",
        "    reference_id=1, image=None, mask_mode=\"foreground\", dilation=0.1\n",
        ")\n",
        "edited_image = edit_model.edit_image(\n",
        "    prompt=edit_prompt,\n",
        "    edit_mode=\"inpainting-insert\",\n",
        "    reference_images=[raw_ref_image, mask_ref_image],\n",
        "    number_of_images=1,\n",
        "    safety_filter_level=\"block_some\",\n",
        "    person_generation=\"allow_adult\",\n",
        ")\n",
        "\n",
        "display_images(generated_image[0], edited_image[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec7135f4de3d"
      },
      "source": [
        "This next example demonstrates another instance of inpainting insert. However, you'll use the semantic mask mode. When using this mask mode, you'll need to specify the class ID of the object in the image that you wish to mask and replace. A list of possible instance types is shown at the end of this notebook. Once you've found the correct segmentation class ID, list it in ```segmentation_classes```.\n",
        "\n",
        "Within the ```MaskReferenceImage``` object you can also configure the dilation value. This float between 0 and 1 represents the percentage of the provided mask. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8pyAJlvQsocc"
      },
      "outputs": [
        {
          "ename": "NotFound",
          "evalue": "404 POST https://asia-east1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/asia-east1/publishers/google/models/imagen-3.0-capability-001@default:predict?%24alt=json%3Benum-encoding%3Dint: Image editing failed with the following error: imagen-3.0-capability-001 is unavailable.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     14\u001b[0m raw_ref_image \u001b[38;5;241m=\u001b[39m RawReferenceImage(image\u001b[38;5;241m=\u001b[39mgenerated_image[\u001b[38;5;241m0\u001b[39m], reference_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     15\u001b[0m mask_ref_image \u001b[38;5;241m=\u001b[39m MaskReferenceImage(\n\u001b[0;32m     16\u001b[0m     reference_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     17\u001b[0m     image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m edited_image \u001b[38;5;241m=\u001b[39m \u001b[43medit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medit_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43medit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minpainting-insert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mraw_ref_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ref_image\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumber_of_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_filter_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblock_some\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperson_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow_adult\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m display_images(generated_image[\u001b[38;5;241m0\u001b[39m], edited_image[\u001b[38;5;241m0\u001b[39m])\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\vertexai\\vision_models\\_vision_models.py:1217\u001b[0m, in \u001b[0;36mImageGenerationModel.edit_image\u001b[1;34m(self, prompt, base_image, mask, reference_images, negative_prompt, number_of_images, guidance_scale, edit_mode, mask_mode, segmentation_classes, mask_dilation, product_position, output_mime_type, compression_quality, language, seed, output_gcs_uri, safety_filter_level, person_generation)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21medit_image\u001b[39m(\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageGenerationResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Edits an existing image based on text prompt.\u001b[39;00m\n\u001b[0;32m   1149\u001b[0m \n\u001b[0;32m   1150\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;124;03m        An `ImageGenerationResponse` object.\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumber_of_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43medit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medit_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msegmentation_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegmentation_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_dilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproduct_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproduct_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_mime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_mime_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression_quality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_quality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_gcs_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gcs_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_watermark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Not supported for editing yet\u001b[39;49;00m\n\u001b[0;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_filter_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_filter_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperson_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperson_generation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\vertexai\\vision_models\\_vision_models.py:1022\u001b[0m, in \u001b[0;36mImageGenerationModel._generate_images\u001b[1;34m(self, prompt, negative_prompt, number_of_images, width, height, aspect_ratio, guidance_scale, seed, base_image, mask, reference_images, edit_mode, mask_mode, segmentation_classes, mask_dilation, product_position, output_mime_type, compression_quality, language, output_gcs_uri, add_watermark, safety_filter_level, person_generation)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_options:\n\u001b[0;32m   1020\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output_options\n\u001b[1;32m-> 1022\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1027\u001b[0m generated_images: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneratedImage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(response\u001b[38;5;241m.\u001b[39mpredictions):\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\google\\cloud\\aiplatform\\models.py:2426\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[1;34m(self, instances, parameters, timeout, use_raw_predict, use_dedicated_endpoint)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[0;32m   2418\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   2419\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2422\u001b[0m         model_version_id\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelVersionId\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   2423\u001b[0m     )\n\u001b[0;32m   2425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2426\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2428\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[0;32m   2433\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json_format\u001b[38;5;241m.\u001b[39mMessageToDict(prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata)\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\google\\cloud\\aiplatform_v1\\services\\prediction_service\\client.py:914\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[1;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Project\\GenAI\\GCP\\newvenv\\Lib\\site-packages\\google\\cloud\\aiplatform_v1\\services\\prediction_service\\transports\\rest.py:1460\u001b[0m, in \u001b[0;36mPredictionServiceRestTransport._Predict.__call__\u001b[1;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;66;03m# In case of error, raise the appropriate core_exceptions.GoogleAPICallError exception\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;66;03m# subclass.\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m-> 1460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core_exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;66;03m# Return the response\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m resp \u001b[38;5;241m=\u001b[39m prediction_service\u001b[38;5;241m.\u001b[39mPredictResponse()\n",
            "\u001b[1;31mNotFound\u001b[0m: 404 POST https://asia-east1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/asia-east1/publishers/google/models/imagen-3.0-capability-001@default:predict?%24alt=json%3Benum-encoding%3Dint: Image editing failed with the following error: imagen-3.0-capability-001 is unavailable."
          ]
        }
      ],
      "source": [
        "image_prompt = \"\"\"\n",
        "a french bulldog sitting in a living room on a couch with green throw pillows and a throw blanket,\n",
        "a circular mirror with a slim black border is on the wall above the couch\n",
        "\"\"\"\n",
        "generated_image = generation_model.generate_images(\n",
        "    prompt=image_prompt,\n",
        "    number_of_images=1,\n",
        "    aspect_ratio=\"1:1\",\n",
        "    safety_filter_level=\"block_some\",\n",
        "    person_generation=\"dont_allow\",\n",
        ")\n",
        "\n",
        "edit_prompt = \"a corgi sitting on a couch\"\n",
        "raw_ref_image = RawReferenceImage(image=generated_image[0], reference_id=0)\n",
        "mask_ref_image = MaskReferenceImage(\n",
        "    reference_id=1,\n",
        "    image=None,\n",
        "    mask_mode=\"semantic\",\n",
        "    segmentation_classes=[8],\n",
        "    dilation=0.1,\n",
        ")\n",
        "edited_image = edit_model.edit_image(\n",
        "    prompt=edit_prompt,\n",
        "    edit_mode=\"inpainting-insert\",\n",
        "    reference_images=[raw_ref_image, mask_ref_image],\n",
        "    number_of_images=1,\n",
        "    safety_filter_level=\"block_some\",\n",
        "    person_generation=\"allow_adult\",\n",
        ")\n",
        "\n",
        "display_images(generated_image[0], edited_image[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ad62258e803"
      },
      "source": [
        "### Inpainting remove\n",
        "\n",
        "Inpainting remove allows you to use a mask area to remove image content. \n",
        "\n",
        "In this next example, you'll take the edited image from the previous cell and create a mask over detected mirror instances. You'll then remove these objects by setting the edit mode to \"inpainting-remove.\" For these types of requests the prompt can be an empty string. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOdKM3ZCB4g6"
      },
      "outputs": [],
      "source": [
        "raw_ref_image = RawReferenceImage(image=edited_image[0], reference_id=0)\n",
        "mask_ref_image = MaskReferenceImage(\n",
        "    reference_id=1, image=None, mask_mode=\"semantic\", segmentation_classes=[85]\n",
        ")\n",
        "remove_image = edit_model.edit_image(\n",
        "    prompt=\"\",\n",
        "    edit_mode=\"inpainting-remove\",\n",
        "    reference_images=[raw_ref_image, mask_ref_image],\n",
        "    number_of_images=1,\n",
        "    safety_filter_level=\"block_some\",\n",
        "    person_generation=\"allow_adult\",\n",
        ")\n",
        "\n",
        "display_images(edited_image[0], remove_image[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68909c926952"
      },
      "source": [
        "### Product background editing via background swap mode\n",
        "\n",
        "\n",
        "You can also use Imagen 3 for product image editing. By setting `edit_mode` to \"background-swap\", you can maintain the product content while modifying the image background.\n",
        "\n",
        "For this example, start with an image stored in a Google Cloud Storage bucket, and provide a prompt describing the new background scene.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5zv8PqYweHW"
      },
      "outputs": [],
      "source": [
        "product_image = Image(\n",
        "    gcs_uri=\"gs://cloud-samples-data/generative-ai/image/suitcase.png\"\n",
        ")\n",
        "raw_ref_image = RawReferenceImage(image=product_image, reference_id=0)\n",
        "mask_ref_image = MaskReferenceImage(reference_id=1, image=None, mask_mode=\"background\")\n",
        "\n",
        "prompt = \"a light blue suitcase in front of a window in an airport, lots of bright, natural lighting coming in from the windows, planes taking off in the distance\"\n",
        "edited_image = edit_model.edit_image(\n",
        "    prompt=prompt,\n",
        "    edit_mode=\"background-swap\",\n",
        "    reference_images=[raw_ref_image, mask_ref_image],\n",
        "    seed=1,\n",
        "    number_of_images=1,\n",
        "    safety_filter_level=\"block_some\",\n",
        "    person_generation=\"allow_adult\",\n",
        ")\n",
        "\n",
        "display_images(product_image, edited_image[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76df73e7bbd2"
      },
      "source": [
        "### Outpainting\n",
        "\n",
        "Imagen 3 editing can be used for image outpainting. Outpainting is used to expand the content of an image to a larger area or area with different dimensions. To use the outpainting feature, you must create an image mask and prepare the original image by padding some empty space around it. Once you've padded the image, you can use the ```outpainting``` editing mode to fill in the empty space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUevoHxxIsIN"
      },
      "outputs": [],
      "source": [
        "initial_image = Image(\n",
        "    gcs_uri=\"gs://cloud-samples-data/generative-ai/image/living-room.png\"\n",
        ")\n",
        "mask = PIL_Image.new(\"L\", initial_image._pil_image.size, 0)\n",
        "\n",
        "target_size_w = int(2500 * eval(\"3/4\"))\n",
        "target_size = (target_size_w, 2500)\n",
        "image_pil_outpaint, mask_pil_outpaint = pad_image_and_mask(\n",
        "    initial_image._pil_image,\n",
        "    mask,\n",
        "    target_size,\n",
        "    0,\n",
        "    0,\n",
        ")\n",
        "\n",
        "raw_ref_image = RawReferenceImage(\n",
        "    image=get_bytes_from_pil(image_pil_outpaint), reference_id=0\n",
        ")\n",
        "mask_ref_image = MaskReferenceImage(\n",
        "    reference_id=1,\n",
        "    image=get_bytes_from_pil(mask_pil_outpaint),\n",
        "    mask_mode=\"user_provided\",\n",
        "    dilation=0.03,\n",
        ")\n",
        "prompt = \"a chandelier hanging from the ceiling\"\n",
        "edited_image = edit_model.edit_image(\n",
        "    prompt=prompt,\n",
        "    edit_mode=\"outpainting\",\n",
        "    reference_images=[raw_ref_image, mask_ref_image],\n",
        "    number_of_images=1,\n",
        "    safety_filter_level=\"block_some\",\n",
        "    person_generation=\"allow_adult\",\n",
        ")\n",
        "\n",
        "display_images(initial_image, edited_image[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "879504c59265"
      },
      "source": [
        "### Controlled editing\n",
        "\n",
        "Controlled editing allows you to turn sketches into a fully realized image. Imagen 3 Controlled Editing also gives you the option to generate a new image guided by a Canny Edge image signal, which is demonstrated in the example below. Generate a new image with Imagen 3, apply the Canny Edge filter, and generate a new image based on this ```ControlReferenceImage```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXKWlllBdcdf"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "generation_prompt = \"\"\"\n",
        "a simple accent chair in a neutral color\n",
        "\"\"\"\n",
        "generated_image = generation_model.generate_images(\n",
        "    prompt=generation_prompt,\n",
        "    number_of_images=1,\n",
        "    aspect_ratio=\"1:1\",\n",
        "    safety_filter_level=\"block_some\",\n",
        "    person_generation=\"dont_allow\",\n",
        ")\n",
        "\n",
        "generated_image[0].save(\"chair.png\")\n",
        "img = cv2.imread(\"chair.png\")\n",
        "\n",
        "# Setting parameter values\n",
        "t_lower = 100  # Lower Threshold\n",
        "t_upper = 150  # Upper threshold\n",
        "\n",
        "# Applying the Canny Edge filter\n",
        "edge = cv2.Canny(img, t_lower, t_upper)\n",
        "cv2.imwrite(\"chair_edge.png\", edge)\n",
        "\n",
        "control_image = ControlReferenceImage(\n",
        "    reference_id=1, image=Image.load_from_file(\"chair_edge.png\"), control_type=\"canny\"\n",
        ")\n",
        "\n",
        "edit_prompt = \"A photorealistic image along the lines of a navy suede accent chair in a living room, near big windows\"\n",
        "\n",
        "control_image = edit_model._generate_images(\n",
        "    prompt=edit_prompt,\n",
        "    number_of_images=1,\n",
        "    aspect_ratio=\"1:1\",\n",
        "    reference_images=[control_image],\n",
        "    safety_filter_level=\"block_some\",\n",
        "    person_generation=\"allow_adult\",\n",
        ")\n",
        "\n",
        "fig, axis = plt.subplots(1, 3, figsize=(12, 6))\n",
        "axis[0].imshow(generated_image[0]._pil_image)\n",
        "axis[0].set_title(\"Original Image\")\n",
        "axis[1].imshow(edge, cmap=\"gray\")\n",
        "axis[1].set_title(\"Canny Edge\")\n",
        "axis[2].imshow(control_image[0]._pil_image)\n",
        "axis[2].set_title(\"Edited Image\")\n",
        "for ax in axis:\n",
        "    ax.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c37c0c51d8f"
      },
      "source": [
        "### Semantic segmentation classes\n",
        "\n",
        "| Class ID | Instance Type | Class ID | Instance Type | Class ID | Instance Type | Class ID | Instance Type |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| 0 | backpack | 50 | carrot | 100 | sidewalk_pavement | 150 | skis |\n",
        "| 1 | umbrella | 51 | hot_dog | 101 | runway | 151 | snowboard |\n",
        "| 2 | bag | 52 | pizza | 102 | terrain | 152 | sports_ball |\n",
        "| 3 | tie | 53 | donut | 103 | book | 153 | kite |\n",
        "| 4 | suitcase | 54 | cake | 104 | box | 154 | baseball_bat |\n",
        "| 5 | case | 55 | fruit_other | 105 | clock | 155 | baseball_glove |\n",
        "| 6 | bird | 56 | food_other | 106 | vase | 156 | skateboard |\n",
        "| 7 | cat | 57 | chair_other | 107 | scissors | 157 | surfboard |\n",
        "| 8 | dog | 58 | armchair | 108 | plaything_other | 158 | tennis_racket |\n",
        "| 9 | horse | 59 | swivel_chair | 109 | teddy_bear | 159 | net |\n",
        "| 10 | sheep | 60 | stool | 110 | hair_dryer | 160 | base |\n",
        "| 11 | cow | 61 | seat | 111 | toothbrush | 161 | sculpture |\n",
        "| 12 | elephant | 62 | couch | 112 | painting | 162 | column |\n",
        "| 13 | bear | 63 | trash_can | 113 | poster | 163 | fountain |\n",
        "| 14 | zebra | 64 | potted_plant | 114 | bulletin_board | 164 | awning |\n",
        "| 15 | giraffe | 65 | nightstand | 115 | bottle | 165 | apparel |\n",
        "| 16 | animal_other | 66 | bed | 116 | cup | 166 | banner |\n",
        "| 17 | microwave | 67 | table | 117 | wine_glass | 167 | flag |\n",
        "| 18 | radiator | 68 | pool_table | 118 | knife | 168 | blanket |\n",
        "| 19 | oven | 69 | barrel | 119 | fork | 169 | curtain_other |\n",
        "| 20 | toaster | 70 | desk | 120 | spoon | 170 | shower_curtain |\n",
        "| 21 | storage_tank | 71 | ottoman | 121 | bowl | 171 | pillow |\n",
        "| 22 | conveyor_belt | 72 | wardrobe | 122 | tray | 172 | towel |\n",
        "| 23 | sink | 73 | crib | 123 | range_hood | 173 | rug_floormat |\n",
        "| 24 | refrigerator | 74 | basket | 124 | plate | 174 | vegetation |\n",
        "| 25 | washer_dryer | 75 | chest_of_drawers | 125 | person | 175 | bicycle |\n",
        "| 26 | fan | 76 | bookshelf | 126 | rider_other | 176 | car |\n",
        "| 27 | dishwasher | 77 | counter_other | 127 | bicyclist | 177 | autorickshaw |\n",
        "| 28 | toilet | 78 | bathroom_counter | 128 | motorcyclist | 178 | motorcycle |\n",
        "| 29 | bathtub | 79 | kitchen_island | 129 | paper | 179 | airplane |\n",
        "| 30 | shower | 80 | door | 130 | streetlight | 180 | bus |\n",
        "| 31 | tunnel | 81 | light_other | 131 | road_barrier | 181 | train |\n",
        "| 32 | bridge | 82 | lamp | 132 | mailbox | 182 | truck |\n",
        "| 33 | pier_wharf | 83 | sconce | 133 | cctv_camera | 183 | trailer |\n",
        "| 34 | tent | 84 | chandelier | 134 | junction_box | 184 | boat_ship |\n",
        "| 35 | building | 85 | mirror | 135 | traffic_sign | 185 | slow_wheeled_object |\n",
        "| 36 | ceiling | 86 | whiteboard | 136 | traffic_light | 186 | river_lake |\n",
        "| 37 | laptop | 87 | shelf | 137 | fire_hydrant | 187 | sea |\n",
        "| 38 | keyboard | 88 | stairs | 138 | parking_meter | 188 | water_other |\n",
        "| 39 | mouse | 89 | escalator | 139 | bench | 189 | swimming_pool |\n",
        "| 40 | remote | 90 | cabinet | 140 | bike_rack | 190 | waterfall |\n",
        "| 41 | cell phone | 91 | fireplace | 141 | billboard | 191 | wall |\n",
        "| 42 | television | 92 | stove | 142 | sky | 192 | window |\n",
        "| 43 | floor | 93 | arcade_machine | 143 | pole | 193 | window_blind |\n",
        "| 44 | stage | 94 | gravel | 144 | fence | | |\n",
        "| 45 | banana | 95 | platform | 145 | railing_banister | | |\n",
        "| 46 | apple | 96 | playingfield | 146 | guard_rail | | |\n",
        "| 47 | sandwich | 97 | railroad | 147 | mountain_hill | | |\n",
        "| 48 | orange | 98 | road | 148 | rock | | |\n",
        "| 49 | broccoli | 99 | snow | 149 | frisbee | | |\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "imagen3_editing.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "newvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
