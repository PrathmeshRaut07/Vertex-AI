{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "import os\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "key_path = r\"stoked-forest-447811-u4-ecf33505a9e7.json\"\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
    "vertexai.init( location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "data_url = \"https://storage.googleapis.com/benchmarks-artifacts/langchain-docs-benchmarking/cj.zip\"\n",
    "result = requests.get(data_url)\n",
    "filename = \"cj.zip\"\n",
    "with open(filename, \"wb\") as file:\n",
    "   file.write(result.content)\n",
    "\n",
    "with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "   zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./cj/cj.pdf\")\n",
    "docs = loader.load()\n",
    "tables = []\n",
    "texts = [d.page_content for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11/14/23, 8:35 PM Clouded Judgement 11.10.23 - by Jamin Ball\\nhttps://cloudedjudgement.substack.com/p/clouded-judgement-111023 1/21\\nClouded Judgement 11.10.23 - OpenAI\\nUpdates + Datadog Gives the All-Clear?\\nJAMIN BALL\\nNOV 10, 2023\\n2 Share\\nEvery week I’ll provide updates on the latest trends in cloud so\\x00ware companies. Follow along to\\nstay up to date!\\nOpenAI Updates\\nOpenAI had their big developer day this week, and I wanted to call out two key announcements\\n(and trends): increasing context windows and decreasing costs.\\nWhen I think about the monetization of AI (and which “layers” monetize \\x00rst) I’ve always\\nthought it would follow the below order, with each layer lagging the one that comes before it.\\n1. Raw silicon (chips like Nvidia bought in large quantities to build out infra to service\\nupcoming demand).\\n2. Model providers (OpenAI, Anthropic, etc as companies start building out AI).\\n35\\nType your email... Subscribe'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI , ChatVertexAI , VertexAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.llms._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised TooManyRequests: 429 POST https://us-central1-aiplatform.googleapis.com/v1/projects/stoked-forest-447811-u4/locations/us-central1/publishers/google/models/gemini-2.0-flash-exp:generateContent?%24alt=json%3Benum-encoding%3Dint: Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Clouded Judgement newsletter, November 10, 2023, by Jamin Ball. Discusses OpenAI developer day announcements including increasing context windows and decreasing costs. Also covers trends in cloud software companies.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "   \"\"\"\n",
    "   Summarize text elements\n",
    "   texts: List of str\n",
    "   tables: List of str\n",
    "   summarize_texts: Bool to summarize texts\n",
    "   \"\"\"\n",
    "\n",
    "   # Prompt\n",
    "   prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "   These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "   Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "   prompt = PromptTemplate.from_template(prompt_text)\n",
    "   empty_response = RunnableLambda(\n",
    "       lambda x: AIMessage(content=\"Error processing document\")\n",
    "   )\n",
    "   # Text summary chain\n",
    "   model = VertexAI(\n",
    "       temperature=0, model_name=\"gemini-2.0-flash-exp\", max_output_tokens=1024\n",
    "   ).with_fallbacks([empty_response])\n",
    "   summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "   # Initialize empty summaries\n",
    "   text_summaries = []\n",
    "   table_summaries = []\n",
    "\n",
    "   # Apply to text if texts are provided and summarization is requested\n",
    "   if texts and summarize_texts:\n",
    "       text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "   elif texts:\n",
    "       text_summaries = texts\n",
    "\n",
    "   # Apply to tables if tables are provided\n",
    "   if tables:\n",
    "       table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
    "\n",
    "   return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "   texts, tables, summarize_texts=True\n",
    ")\n",
    "\n",
    "text_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_9544\\2448046868.py:11: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  msg = model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The image shows a line chart of EV/NTM Revenue Multiples over time. The three lines represent different growth rates: high, mid, and low. The chart shows that EV/NTM Revenue Multiples have been increasing over time, with the high growth rate line showing the highest increase.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_image(image_path):\n",
    "   \"\"\"Getting the base64 string\"\"\"\n",
    "   with open(image_path, \"rb\") as image_file:\n",
    "       return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "   \"\"\"Make image summary\"\"\"\n",
    "   model = ChatVertexAI(model_name=\"gemini-pro-vision\", max_output_tokens=1024)\n",
    "\n",
    "   msg = model(\n",
    "       [\n",
    "           HumanMessage(\n",
    "               content=[\n",
    "                   {\"type\": \"text\", \"text\": prompt},\n",
    "                   {\n",
    "                       \"type\": \"image_url\",\n",
    "                       \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                   },\n",
    "               ]\n",
    "           )\n",
    "       ]\n",
    "   )\n",
    "   return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "   \"\"\"\n",
    "   Generate summaries and base64 encoded strings for images\n",
    "   path: Path to list of .jpg files extracted by Unstructured\n",
    "   \"\"\"\n",
    "\n",
    "   # Store base64 encoded images\n",
    "   img_base64_list = []\n",
    "\n",
    "   # Store image summaries\n",
    "   image_summaries = []\n",
    "\n",
    "   # Prompt\n",
    "   prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "   These summaries will be embedded and used to retrieve the raw image. \\\n",
    "   Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "   # Apply to images\n",
    "   for img_file in sorted(os.listdir(path)):\n",
    "       if img_file.endswith(\".jpg\"):\n",
    "           img_path = os.path.join(path, img_file)\n",
    "           base64_image = encode_image(img_path)\n",
    "           img_base64_list.append(base64_image)\n",
    "           image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "   return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(\"./cj\")\n",
    "\n",
    "len(img_base64_list)\n",
    "\n",
    "len(image_summaries)\n",
    "\n",
    "image_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(\n",
    "   vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "   \"\"\"\n",
    "   Create retriever that indexes summaries, but returns raw images or texts\n",
    "   \"\"\"\n",
    "\n",
    "   # Initialize the storage layer\n",
    "   store = InMemoryStore()\n",
    "   id_key = \"doc_id\"\n",
    "\n",
    "   # Create the multi-vector retriever\n",
    "   retriever = MultiVectorRetriever(\n",
    "       vectorstore=vectorstore,\n",
    "       docstore=store,\n",
    "       id_key=id_key,\n",
    "   )\n",
    "\n",
    "   # Helper function to add documents to the vectorstore and docstore\n",
    "   def add_documents(retriever, doc_summaries, doc_contents):\n",
    "       doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "       summary_docs = [\n",
    "           Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "           for i, s in enumerate(doc_summaries)\n",
    "       ]\n",
    "       retriever.vectorstore.add_documents(summary_docs)\n",
    "       retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "   # Add texts, tables, and images\n",
    "   # Check that text_summaries is not empty before adding\n",
    "   if text_summaries:\n",
    "       add_documents(retriever, text_summaries, texts)\n",
    "   # Check that table_summaries is not empty before adding\n",
    "   if table_summaries:\n",
    "       add_documents(retriever, table_summaries, tables)\n",
    "   # Check that image_summaries is not empty before adding\n",
    "   if image_summaries:\n",
    "       add_documents(retriever, image_summaries, images)\n",
    "\n",
    "   return retriever\n",
    "\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "   collection_name=\"mm_rag_cj_blog\",\n",
    "   embedding_function=VertexAIEmbeddings(model_name=\"textembedding-gecko@latest\"),\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "   vectorstore,\n",
    "   text_summaries,\n",
    "   texts,\n",
    "   table_summaries,\n",
    "   tables,\n",
    "   image_summaries,\n",
    "   img_base64_list,\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "   \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "   # Create an HTML img tag with the base64 string as the source\n",
    "   image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "   # Display the image by rendering the HTML\n",
    "   display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "   \"\"\"Check if the string looks like base64\"\"\"\n",
    "   return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "   \"\"\"\n",
    "   Check if the base64 data is an image by looking at the start of the data\n",
    "   \"\"\"\n",
    "   image_signatures = {\n",
    "       b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "       b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "       b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "       b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "   }\n",
    "   try:\n",
    "       header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "       for sig, format in image_signatures.items():\n",
    "           if header.startswith(sig):\n",
    "               return True\n",
    "       return False\n",
    "   except Exception:\n",
    "       return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "   \"\"\"\n",
    "   Resize an image encoded as a Base64 string\n",
    "   \"\"\"\n",
    "   # Decode the Base64 string\n",
    "   img_data = base64.b64decode(base64_string)\n",
    "   img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "   # Resize the image\n",
    "   resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "   # Save the resized image to a bytes buffer\n",
    "   buffered = io.BytesIO()\n",
    "   resized_img.save(buffered, format=img.format)\n",
    "\n",
    "   # Encode the resized image to Base64\n",
    "   return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "   \"\"\"\n",
    "   Split base64-encoded images and texts\n",
    "   \"\"\"\n",
    "   b64_images = []\n",
    "   texts = []\n",
    "   for doc in docs:\n",
    "       # Check if the document is of type Document and extract page_content if so\n",
    "       if isinstance(doc, Document):\n",
    "           doc = doc.page_content\n",
    "       if looks_like_base64(doc) and is_image_data(doc):\n",
    "           doc = resize_base64_image(doc, size=(1300, 600))\n",
    "           b64_images.append(doc)\n",
    "       else:\n",
    "           texts.append(doc)\n",
    "   if len(b64_images) > 0:\n",
    "       return {\"images\": b64_images[:1], \"texts\": []}\n",
    "   return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_prompt_func(data_dict):\n",
    "   \"\"\"\n",
    "   Join the context into a single string\n",
    "   \"\"\"\n",
    "   formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "   messages = []\n",
    "\n",
    "   # Adding the text for analysis\n",
    "   text_message = {\n",
    "       \"type\": \"text\",\n",
    "       \"text\": (\n",
    "           \"You are financial analyst tasking with providing investment advice.\\n\"\n",
    "           \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "           \"Use this information to provide investment advice related to the user question. \\n\"\n",
    "           f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "           \"Text and / or tables:\\n\"\n",
    "           f\"{formatted_texts}\"\n",
    "       ),\n",
    "   }\n",
    "   messages.append(text_message)\n",
    "   # Adding image(s) to the messages if present\n",
    "   if data_dict[\"context\"][\"images\"]:\n",
    "       for image in data_dict[\"context\"][\"images\"]:\n",
    "           image_message = {\n",
    "               \"type\": \"image_url\",\n",
    "               \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "           }\n",
    "           messages.append(image_message)\n",
    "   return [HumanMessage(content=messages)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_rag_chain(retriever):\n",
    "   \"\"\"\n",
    "   Multi-modal RAG chain\n",
    "   \"\"\"\n",
    "\n",
    "   # Multi-modal LLM\n",
    "   model = ChatVertexAI(\n",
    "       temperature=0, model_name=\"gemini-pro-vision\", max_output_tokens=1024\n",
    "   )\n",
    "\n",
    "   # RAG pipeline\n",
    "   chain = (\n",
    "       {\n",
    "           \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "           \"question\": RunnablePassthrough(),\n",
    "       }\n",
    "       | RunnableLambda(img_prompt_func)\n",
    "       | model\n",
    "       | StrOutputParser()\n",
    "   )\n",
    "\n",
    "   return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_9544\\2171678302.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever_multi_vector_img.get_relevant_documents(query, limit=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['11/14/23, 8:35 PM Clouded Judgement 11.10.23 - by Jamin Ball\\nhttps://cloudedjudgement.substack.com/p/clouded-judgement-111023 15/21\\nScatter Plot of EV / NTM Rev Multiple vs NTM Rev Growth\\nHow correlated is growth to valuation multiple?\\n',\n",
       " '11/14/23, 8:35 PM Clouded Judgement 11.10.23 - by Jamin Ball\\nhttps://cloudedjudgement.substack.com/p/clouded-judgement-111023 16/21\\nOperating Metrics\\nMedian NTM growth rate: 15%\\nMedian LTM growth rate: 21%\\nMedian Gross Margin: 75%\\nMedian Operating Margin (18%)\\nMedian FCF Margin: 8%\\nMedian Net Retention: 114%\\n',\n",
       " \"11/14/23, 8:35 PM Clouded Judgement 11.10.23 - by Jamin Ball\\nhttps://cloudedjudgement.substack.com/p/clouded-judgement-111023 4/21\\n“It looks like we've hit an in\\x00ection point. It looks like there's a lot less overhang now in\\nterms of what needs to be optimized or could be optimized by customers. It looks like\\nalso optimization is less intense and less widespread across the customer base.”\\n“We had a very healthy start to Q4 in October...the trends we see in early Q4 are stronger\\nthan they've been for the past year.”\\n“As we look at our overall customer activity, we continue to see customers optimizing\\nbut with less impact than we experienced in Q2, contributing to our usage growth with\\nexisting customers improving in Q3 relative to Q2.”\\n“As a reminder, last quarter, we discussed a cohort of customers who began optimizing\\nabout a year ago and we said that they appear to stabilize their users growth at the end of\\nQ2. That trend has held for the past several months with that cohorts usage remaining\\nstable throughout Q3.”\\nDatadog was one of the \\x00rst companies to really highlight an improving macro environment.\\nAnd even more important, they called out a great month of October (\\x00rst month of Q4 for them).\\nSo how do we contrast their positive commentary, with largely neutral commentary from the rest\\nof the so\\x00ware universe? Most likely Datadog is seeing trends more unique to their own\\nbusiness. As the market puts a greater emphasis on bundled platforms today vs point solutions,\\nthey appear to be an incremental winner of market share. Best of breed platforms (with more of a\\nusage based model) will recover \\x00rst (in terms of revenue growth recovery). Datadog appears to\\nbe in that bucket and recovering \\x00rst. This doesn’t mean the rest of the so\\x00ware universe will\\nfollow suite. There will be many “pretenders” who never recover and \\x00nd themselves bundled\\ninto oblivion. However, the positive commentary from Datadog is the \\x00rst sign that we’re\",\n",
       " '11/14/23, 8:35 PM Clouded Judgement 11.10.23 - by Jamin Ball\\nhttps://cloudedjudgement.substack.com/p/clouded-judgement-111023 11/21\\nEV / NTM Rev / NTM Growth\\nThe below chart shows the EV / NTM revenue multiple divided by NTM consensus growth\\nexpectations. So a company trading at 20x NTM revenue that is projected to grow 100% would be\\ntrading at 0.2x. The goal of this graph is to show how relatively cheap / expensive each stock is\\nrelative to their growth expectations\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the EV / NTM and NTM rev growth for MongoDB, Cloudflare, and Datadog?\"\n",
    "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=1)\n",
    "\n",
    "# We get relevant docs\n",
    "len(docs)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I apologize, but the provided text and tables do not contain the EV / NTM and NTM rev growth for MongoDB, Cloudflare, and Datadog. Therefore, I cannot provide the requested investment advice."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain_multimodal_rag.invoke(query)\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "md(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
